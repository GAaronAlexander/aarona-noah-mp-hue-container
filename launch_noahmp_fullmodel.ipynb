{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task-utils for computing CSG metrics from Climate Impacts Lab downscaled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from jupiter.task_utils import TaskSet, TaskInventory, TaskInstructions, TaskLauncher\n",
    "from jupiter.aws.s3 import upload_s3_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some parameters\n",
    "\n",
    "params = {\n",
    "    # Path to inventory file\n",
    "    'save_location':'s3://jupiter-intern-projects/aarona/noah-mp-hue/${domain}/${start_date}_${end_date}/results',\n",
    "    'ICBC_location':'s3://jupiter-intern-projects/aarona/noah-mp-hue/${domain}/${start_date}_${end_date}/ICBC',\n",
    "    'domain': 'milwaukee',\n",
    "    'geogrid_file':'/home/jupiter/model/noahmp/geogrid-files/geo_em.d01.${domain}.nc',\n",
    "    'number_runday':'2',\n",
    "    'LSM_timestep':'1800',\n",
    "    'output_timestep':'1800',\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "batch_queue = 'csp-dev-wrf' #special queue for me\n",
    "taskset_name = 'test-runs-fullsim' # this is called aarona-noah-mp-hue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find expected output files\n",
    "Load the inventory file to figure out how many output files we need to loop over/should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = ['2010-06-01','2019-04-01']\n",
    "end_dates   = ['2010-08-01','2019-08-10']\n",
    "start_hours = ['00','00']\n",
    "\n",
    "params['start_dates'] = start_dates\n",
    "params['end_dates'] = end_dates\n",
    "params['start_hours'] = start_hours\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting task-utils YAML\n",
    "\n",
    "This dictionary should include the key components to the task-utils yaml file.  The lists of download parameters from the dataframe above are programmatically inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /home/jupiter/model/noahmp/run-noahmp-era5.py --ICBC-location ${ICBC_location} --number-runday ${number_runday} --LSM-timestep ${LSM_timestep} -- output-timestep ${output_timestep} --save-location ${save_location} --start-date ${start_date} --start-hour ${start_hour} --geogrid-file ${geogrid_file}\n"
     ]
    }
   ],
   "source": [
    "cmd_str = 'python /home/jupiter/model/noahmp/run-noahmp-era5.py --ICBC-location ${ICBC_location} --number-runday ${number_runday} --LSM-timestep ${LSM_timestep} -- output-timestep ${output_timestep} --save-location ${save_location} --start-date ${start_date} --start-hour ${start_hour} --geogrid-file ${geogrid_file}'\n",
    "print(cmd_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\n",
    "    'name' : taskset_name,\n",
    "    'labels' : {\n",
    "        'env' : 'dev',\n",
    "        'project' : 'csg',\n",
    "        'S3_ROOT' : 's3://jupiter-intern-projects/aarona', #going to point to my s3 space\n",
    "    },\n",
    "    'definitions' : params,\n",
    "    'launch_settings' : {\n",
    "        'batch' : {\n",
    "            #'job_def':f'eos-external-data-etl:3',\n",
    "            'job_def': f'aarona-noah-mp-hue:1',\n",
    "            'queue' : batch_queue,\n",
    "            #'overrides' : {\n",
    "            #    'vcpus' : 4,\n",
    "            #    'memory' : 16000,\n",
    "            #},\n",
    "            'overrides' : {\n",
    "                \"resourceRequirements\": [\n",
    "                    {\n",
    "                        \"type\": \"MEMORY\",\n",
    "                        \"value\" : \"16000\" #MB\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"VCPU\",\n",
    "                        \"value\" : \"4\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'run_keys' : {\n",
    "            'command_string' : cmd_str\n",
    "        }\n",
    "    },\n",
    "    'indicators' : {\n",
    "        #'completed' : {\n",
    "        #    'components' : ['${outpath}'],\n",
    "        #   'method' : 's3_sensor'\n",
    "       # },\n",
    "    },\n",
    "    'loops': {\n",
    "        'start_date' : 'start_dates',\n",
    "    },\n",
    "    'mapped_loops' : {\n",
    "        'start_hour' : {'start_date' : 'start_hours'},\n",
    "        'end_date' : {'start_date' : 'end_dates'},\n",
    "       # 'gcm' : {'job_index' : 'gcms'},\n",
    "       # 'scenario' : {'job_index' : 'scenarios'},\n",
    "       # 'year' : {'job_index' : 'years'}\n",
    "    }\n",
    "        \n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a yaml file\n",
    "with open(f'{taskset_name}.yaml', 'w') as outfile:\n",
    "    docs = yaml.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with task-utils\n",
    "\n",
    "This follows the normal task-utils sequence.  Start by making an instruction list (which should also help confirm our YAML was formatted correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-21 19:57:25,111 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-21 19:57:25,221 | DEBUG      | jupiter.aws.s3_logger.S3Logger:check_s3_object_exists:392 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/spec_files/test-runs-fullsim.yaml exists\n",
      "2022-07-21 19:57:25,222 | WARNING    | jupiter.task_utils.task_set.TaskSet:_sync:389 | Spec file already exists on S3 (s3://jupiter-intern-projects/aarona/task_utils/spec_files/test-runs-fullsim.yaml); overwriting...\n"
     ]
    }
   ],
   "source": [
    "DRY_RUN = False# if True, no batch runs will actually be launched\n",
    "\n",
    "\n",
    "ts = TaskSet(f'{taskset_name}.yaml',sync='overwrite')\n",
    "instr = TaskInstructions(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>S3_ROOT</th>\n",
       "      <th>env</th>\n",
       "      <th>project</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>s3://jupiter-intern-projects/aarona</td>\n",
       "      <td>dev</td>\n",
       "      <td>csg</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>s3://jupiter-intern-projects/aarona</td>\n",
       "      <td>dev</td>\n",
       "      <td>csg</td>\n",
       "      <td>2019-08-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_date                              S3_ROOT  env project    end_date  \\\n",
       "0  2010-06-01  s3://jupiter-intern-projects/aarona  dev     csg  2010-08-01   \n",
       "1  2019-04-01  s3://jupiter-intern-projects/aarona  dev     csg  2019-08-10   \n",
       "\n",
       "  start_hour  \n",
       "0         00  \n",
       "1         00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-21 19:57:29,179 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-fullsim.csv exists\n",
      "2022-07-21 19:57:29,186 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:assign_basic_test_set:641 | Assigned the first 5 entries as the test set\n",
      "2022-07-21 19:57:29,309 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-fullsim.csv\n",
      "2022-07-21 19:57:29,333 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:filter_tests_only:218 | Subsetting instructions to only run tasks flagged as tests\n",
      "2022-07-21 19:57:29,359 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-fullsim.csv exists\n",
      "2022-07-21 19:57:29,397 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-fullsim.csv\n",
      "2022-07-21 19:57:29,401 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:preview:239 | Generating preview of test-runs-fullsim\n",
      "command_string: python /home/jupiter/model/noahmp/run-noahmp-era5.py --ICBC-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2010-06-01_2010-08-01/ICBC --number-runday 2 --LSM-timestep 1800 -- output-timestep 1800 --save-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2010-06-01_2010-08-01/results --start-date 2010-06-01 --start-hour 00 --geogrid-file /home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc\n",
      "command_string: python /home/jupiter/model/noahmp/run-noahmp-era5.py --ICBC-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2019-04-01_2019-08-10/ICBC --number-runday 2 --LSM-timestep 1800 -- output-timestep 1800 --save-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2019-04-01_2019-08-10/results --start-date 2019-04-01 --start-hour 00 --geogrid-file /home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc\n",
      "Specified batch infrastructure: queue csp-dev-wrf, job-def aarona-noah-mp-hue:1\n",
      "2 commands\n",
      "Notebook DRY_RUN value is set to False\n"
     ]
    }
   ],
   "source": [
    "# use this for the basic testing mode; just do the first X runs\n",
    "test_mode = 'basic'\n",
    "number_of_tests = 5\n",
    "\n",
    "# use this if you want your tests to comprise of one run for each value of a specified loop (or multiple loops)\n",
    "#test_mode = \"one_per_loop\"\n",
    "#loop_names = [\"peril\"]\n",
    "\n",
    "# use this if you want to specify a custom query to define your test set\n",
    "#test_mode = \"query\"\n",
    "#test_query = f\"(tileid in {testset}) and (projection_scenario == 'ssp585') and (metric == 'windSpeed500yr')\"#\" and scenario == 'worst_one'\"\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "load_existing_inventory_for_tests = False\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory_for_tests)\n",
    "if test_mode == 'basic': task_inv.assign_basic_test_set(number_in_set = number_of_tests)\n",
    "elif test_mode == 'one_per_loop': task_inv.assign_test_set_by_loop(loops=loop_names, combinations=True)\n",
    "elif test_mode == 'query': task_inv.assign_test_set_by_query(query_str=test_query)\n",
    "else: raise ValueError(f\"test_mode {test_mode} not valid\")\n",
    "task_inv.save()\n",
    "\n",
    "task_instructions_test = TaskInstructions(ts)\n",
    "task_instructions_test.filter_tests_only()\n",
    "task_instructions_test.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch test jobs here\n",
    "\n",
    "Please use job arrays to make this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:30:02,290 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:write_instructions:353 | Wrote instruction_file to s3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_test-runs-bc_20220713_173002.yaml\n",
      "2022-07-13 17:30:02,399 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:launch_via_aws_job_array:399 | Preparing to launch test-runs-bc from s3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_test-runs-bc_20220713_173002.yaml as 1 job arrays of average length 2.0 for a total of 2 jobs using Batch queue csp-dev-wrf and job definition aarona-noah-mp-hue:1.\n",
      "2022-07-13 17:30:02,510 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_submit_batch_job_array:716 | Submitting job array test-runs-bc to queue csp-dev-wrf with job definition aarona-noah-mp-hue:1\n",
      "2022-07-13 17:30:02,511 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_batch_job_array:719 | Submission: {'jobName': 'test-runs-bc', 'jobQueue': 'csp-dev-wrf', 'jobDefinition': 'aarona-noah-mp-hue:1', 'arrayProperties': {'size': 2}, 'containerOverrides': ordereddict([('resourceRequirements', [ordereddict([('type', 'MEMORY'), ('value', '16000')]), ordereddict([('type', 'VCPU'), ('value', '4')])]), ('environment', [{'name': 'JUPITER_TASK_SET', 'value': 's3://jupiter-intern-projects/aarona/task_utils/spec_files/test-runs-bc.yaml'}, {'name': 'JUPITER_TASK_INSTRUCTIONS', 'value': 's3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_test-runs-bc_20220713_173002.yaml'}]), ('command', ['python', '/home/jupiter/model/noahmp/generate-era5-boundary-conditions.py', '--job-array', 's3://jupiter-intern-projects/aarona/task_utils/job_array_files/test-runs-bc_20220713_173002_0.txt'])])}\n",
      "2022-07-13 17:30:02,772 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_batch_job_array:721 | Got response: {'ResponseMetadata': {'RequestId': 'bbe6249d-aa93-4b46-8d91-19070e3bf68b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 13 Jul 2022 17:30:02 GMT', 'content-type': 'application/json', 'content-length': '162', 'connection': 'keep-alive', 'x-amzn-requestid': 'bbe6249d-aa93-4b46-8d91-19070e3bf68b', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'VN0cMH11IAMFhYg=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-62cf011a-22dd8c5f56346b4a34c5505e'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:us-east-1:660097632732:job/9ef33bdd-2ee1-41f4-a71f-ceaef810f8e8', 'jobName': 'test-runs-bc', 'jobId': '9ef33bdd-2ee1-41f4-a71f-ceaef810f8e8'}\n",
      "2022-07-13 17:30:03,022 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:mark_as_launched:860 | Wrote log record to s3://jupiter-intern-projects/aarona/task_utils/launch_records/20220713_173002_test-runs-bc.txt\n",
      "2022-07-13 17:30:03,123 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-13 17:30:03,124 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_sync:356 | TaskSet loaded from official S3 location; no sync needed\n",
      "2022-07-13 17:30:03,183 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv exists\n",
      "2022-07-13 17:30:03,265 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv\n",
      "2022-07-13 17:30:03,267 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_record_job_ids:153 | Updating TaskInventory (s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv) with job ids\n",
      "2022-07-13 17:30:03,326 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv\n"
     ]
    }
   ],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None #'peril'\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# See step 4 for details\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file_test = task_instructions_test.write_instructions()\n",
    "tl_test = TaskLauncher(instructions_file_test)\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:33:47,228 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv exists\n",
      "2022-07-13 17:33:47,314 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv\n",
      "2022-07-13 17:33:47,316 | WARNING    | jupiter.task_utils.task_inventory.TaskInventory:update_status:302 | No indicators were included with the input TaskSet, so there is no way to update status.\n",
      "2022-07-13 17:33:47,317 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:455 | Updating Batch job status for test-runs-bc\n",
      "2022-07-13 17:33:47,328 | DEBUG      | jupiter.task_utils.task_inventory.TaskInventory:_query_batch_status:506 | Check batch status for jobs 0:100\n",
      "2022-07-13 17:33:47,427 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:457 | Batch job status update complete!\n",
      "2022-07-13 17:33:47,432 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:414 | ---------------------------------------------------------------\n",
      "2022-07-13 17:33:47,433 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:415 | Summary of TESTS for test-runs-bc as of 2022-07-13 17:33:47 UTC\n",
      "2022-07-13 17:33:47,434 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:418 | Out of 2 tests...\n",
      "2022-07-13 17:33:47,435 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:427 | ---------------------------------------------------------------\n",
      "2022-07-13 17:33:47,438 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:601 | ---------------------------------------------------------------\n",
      "2022-07-13 17:33:47,439 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:602 | Batch Status of TESTS for test-runs-bc as of 2022-07-13 17:33:47 UTC\n",
      "2022-07-13 17:33:47,440 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:605 | Out of 2 tests...\n",
      "2022-07-13 17:33:47,446 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:620 | FAILED: 2 (100.0%) [0 spot terminations]\n",
      "2022-07-13 17:33:47,447 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:621 | ---------------------------------------------------------------\n",
      "2022-07-13 17:33:47,484 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/test-runs-bc.csv\n"
     ]
    }
   ],
   "source": [
    "# No options here, just execute it!\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {\"completed\":1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = [\"SUCCEEDED\"]\n",
    "num_processors = None\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses, tests_only = True)\n",
    "task_inv.print_summary(tests_only = True)\n",
    "if also_update_batch_status: task_inv.print_batch_summary(tests_only = True)\n",
    "task_inv.save()\n",
    "#task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_bcc01_row0_col0,#T_bcc01_row0_col1,#T_bcc01_row0_col2,#T_bcc01_row0_col3,#T_bcc01_row0_col4,#T_bcc01_row0_col5,#T_bcc01_row0_col6,#T_bcc01_row0_col7,#T_bcc01_row0_col8,#T_bcc01_row0_col9,#T_bcc01_row1_col0,#T_bcc01_row1_col1,#T_bcc01_row1_col2,#T_bcc01_row1_col3,#T_bcc01_row1_col4,#T_bcc01_row1_col5,#T_bcc01_row1_col6,#T_bcc01_row1_col7,#T_bcc01_row1_col8,#T_bcc01_row1_col9{\n",
       "            background-color:  #ff8f82;\n",
       "        }</style><table id=\"T_bcc01_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >start_date</th>        <th class=\"col_heading level0 col1\" >end_date</th>        <th class=\"col_heading level0 col2\" >LAST_KNOWN_JOB_STATUS</th>        <th class=\"col_heading level0 col3\" >CLOUDWATCH_LOGS</th>        <th class=\"col_heading level0 col4\" >JOB_STATUS_REASON</th>        <th class=\"col_heading level0 col5\" >SPOT_TERMINATED</th>        <th class=\"col_heading level0 col6\" >IN_TEST_SET</th>        <th class=\"col_heading level0 col7\" >LAST_KNOWN_JOB_ID</th>        <th class=\"col_heading level0 col8\" >LAST_JOB_CHECK</th>        <th class=\"col_heading level0 col9\" >JOB_DURATION</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_bcc01_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_bcc01_row0_col0\" class=\"data row0 col0\" >2018-04-01</td>\n",
       "                        <td id=\"T_bcc01_row0_col1\" class=\"data row0 col1\" >2018-10-01</td>\n",
       "                        <td id=\"T_bcc01_row0_col2\" class=\"data row0 col2\" >FAILED</td>\n",
       "                        <td id=\"T_bcc01_row0_col3\" class=\"data row0 col3\" ><a target=\"_blank\" href=\"https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252F9a167c2522104009a1133d532de65771\">link</a></td>\n",
       "                        <td id=\"T_bcc01_row0_col4\" class=\"data row0 col4\" >Essential container in task exited</td>\n",
       "                        <td id=\"T_bcc01_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "                        <td id=\"T_bcc01_row0_col6\" class=\"data row0 col6\" >✓</td>\n",
       "                        <td id=\"T_bcc01_row0_col7\" class=\"data row0 col7\" >9ef33bdd-2ee1-41f4-a71f-ceaef810f8e8:0</td>\n",
       "                        <td id=\"T_bcc01_row0_col8\" class=\"data row0 col8\" >2022-07-13T17:33:47</td>\n",
       "                        <td id=\"T_bcc01_row0_col9\" class=\"data row0 col9\" >00:00:00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bcc01_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_bcc01_row1_col0\" class=\"data row1 col0\" >2019-04-01</td>\n",
       "                        <td id=\"T_bcc01_row1_col1\" class=\"data row1 col1\" >2019-10-10</td>\n",
       "                        <td id=\"T_bcc01_row1_col2\" class=\"data row1 col2\" >FAILED</td>\n",
       "                        <td id=\"T_bcc01_row1_col3\" class=\"data row1 col3\" ><a target=\"_blank\" href=\"https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252Fbcaffd3f450e4d1db16dd7910a878847\">link</a></td>\n",
       "                        <td id=\"T_bcc01_row1_col4\" class=\"data row1 col4\" >Essential container in task exited</td>\n",
       "                        <td id=\"T_bcc01_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "                        <td id=\"T_bcc01_row1_col6\" class=\"data row1 col6\" >✓</td>\n",
       "                        <td id=\"T_bcc01_row1_col7\" class=\"data row1 col7\" >9ef33bdd-2ee1-41f4-a71f-ceaef810f8e8:1</td>\n",
       "                        <td id=\"T_bcc01_row1_col8\" class=\"data row1 col8\" >2022-07-13T17:33:47</td>\n",
       "                        <td id=\"T_bcc01_row1_col9\" class=\"data row1 col9\" >00:00:00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3841800ee0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252F9a167c2522104009a1133d532de65771'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.test_df['CLOUDWATCH_LOGS'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'completed != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"] and IN_TEST_SET == True'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS == \"FAILED\" and IN_TEST_SET == True and completed != 1'\n",
    "\n",
    "update_first = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "#rerun_instructions.filter_tests_only()\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_instructions_file_test = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file_test)\n",
    "use_job_arrays = False\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "use_job_arrays = True\n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR RUNNING ALL TASKS\n",
    "## Create instructions for all remaining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create instructions and verify preview\n",
    "task_instructions = TaskInstructions(ts)\n",
    "task_instructions.filter_tests_excluded()  # comment this line if you want to run EVERYTHING, even previous tests\n",
    "#task_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch all runs\n",
    "Please use job arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 5\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file = task_instructions.write_instructions()\n",
    "tl = TaskLauncher(instructions_file)\n",
    "\n",
    "if use_job_arrays: tl.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "task_inv.print_summary()\n",
    "if also_update_batch_status: task_inv.print_batch_summary()\n",
    "task_inv.save()\n",
    "#.styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_inv.apply_style(task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED']['tileid'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'succeeded != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"]'\n",
    "#filter_query = '(completed != 1)'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS in [\"FAILED\"]'\n",
    "\n",
    "update_first = False\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = False\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rerun_instructions_file = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file)\n",
    "\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "    \n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
