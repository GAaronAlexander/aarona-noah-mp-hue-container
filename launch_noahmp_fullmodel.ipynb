{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task-utils for computing CSG metrics from Climate Impacts Lab downscaled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from jupiter.task_utils import TaskSet, TaskInventory, TaskInstructions, TaskLauncher\n",
    "from jupiter.aws.s3 import upload_s3_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some parameters\n",
    "\n",
    "params = {\n",
    "    # Path to inventory file\n",
    "    'save_location':'s3://jupiter-intern-projects/aarona/noah-mp-hue/${domain}/${start_date}_${end_date}/results',\n",
    "    'ICBC_location':'s3://jupiter-intern-projects/aarona/noah-mp-hue/${domain}/${start_date}_${end_date}/ICBC',\n",
    "    'domain': 'milwaukee',\n",
    "    'geogrid_file':'/home/jupiter/model/noahmp/geogrid-files/geo_em.d01.${domain}.nc',\n",
    "    'number_runday':'245',\n",
    "    'LSM_timestep':'900', #adjusted because snow is tricky\n",
    "    'output_timestep':'1800',\n",
    "    'num_cpus':'9',\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "batch_queue = 'csp-dev-wrf' #special queue for me\n",
    "taskset_name = 'milwaukee-full-sim-2008' # this is called aarona-noah-mp-hue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find expected output files\n",
    "Load the inventory file to figure out how many output files we need to loop over/should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = ['2008-03-01']\n",
    "end_dates   = ['2008-11-02']\n",
    "start_hours = ['00']\n",
    "\n",
    "params['start_dates'] = start_dates\n",
    "params['end_dates'] = end_dates\n",
    "params['start_hours'] = start_hours\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting task-utils YAML\n",
    "\n",
    "This dictionary should include the key components to the task-utils yaml file.  The lists of download parameters from the dataframe above are programmatically inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /home/jupiter/model/noahmp/run-noahmp-era5.py --num-cpus ${num_cpus} --ICBC-location ${ICBC_location} --number-runday ${number_runday} --LSM-timestep ${LSM_timestep} --output-timestep ${output_timestep} --save-location ${save_location} --start-date ${start_date} --start-hour ${start_hour} --geogrid-file ${geogrid_file}\n"
     ]
    }
   ],
   "source": [
    "cmd_str = 'python /home/jupiter/model/noahmp/run-noahmp-era5.py --num-cpus ${num_cpus} --ICBC-location ${ICBC_location} --number-runday ${number_runday} --LSM-timestep ${LSM_timestep} --output-timestep ${output_timestep} --save-location ${save_location} --start-date ${start_date} --start-hour ${start_hour} --geogrid-file ${geogrid_file}'\n",
    "print(cmd_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\n",
    "    'name' : taskset_name,\n",
    "    'labels' : {\n",
    "        'env' : 'dev',\n",
    "        'project' : 'csg',\n",
    "        'S3_ROOT' : 's3://jupiter-intern-projects/aarona', #going to point to my s3 space\n",
    "    },\n",
    "    'definitions' : params,\n",
    "    'launch_settings' : {\n",
    "        'batch' : {\n",
    "            #'job_def':f'eos-external-data-etl:3',\n",
    "            'job_def': f'aarona-noah-mp-hue:1',\n",
    "            'queue' : batch_queue,\n",
    "            #'overrides' : {\n",
    "            #    'vcpus' : 4,\n",
    "            #    'memory' : 16000,\n",
    "            #},\n",
    "            'overrides' : {\n",
    "                \"resourceRequirements\": [\n",
    "                    {\n",
    "                        \"type\": \"MEMORY\",\n",
    "                        \"value\" : \"140000\" #MB\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"VCPU\",\n",
    "                        \"value\" : \"72\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'run_keys' : {\n",
    "            'command_string' : cmd_str\n",
    "        }\n",
    "    },\n",
    "    'indicators' : {\n",
    "        #'completed' : {\n",
    "        #    'components' : ['${outpath}'],\n",
    "        #   'method' : 's3_sensor'\n",
    "       # },\n",
    "    },\n",
    "    'loops': {\n",
    "        'start_date' : 'start_dates',\n",
    "    },\n",
    "    'mapped_loops' : {\n",
    "        'start_hour' : {'start_date' : 'start_hours'},\n",
    "        'end_date' : {'start_date' : 'end_dates'},\n",
    "       # 'gcm' : {'job_index' : 'gcms'},\n",
    "       # 'scenario' : {'job_index' : 'scenarios'},\n",
    "       # 'year' : {'job_index' : 'years'}\n",
    "    }\n",
    "        \n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a yaml file\n",
    "with open(f'{taskset_name}.yaml', 'w') as outfile:\n",
    "    docs = yaml.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with task-utils\n",
    "\n",
    "This follows the normal task-utils sequence.  Start by making an instruction list (which should also help confirm our YAML was formatted correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-25 16:42:44,692 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-25 16:42:44,753 | DEBUG      | jupiter.aws.s3_logger.S3Logger:check_s3_object_exists:392 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/spec_files/milwaukee-full-sim-2008.yaml exists\n",
      "2022-07-25 16:42:44,754 | WARNING    | jupiter.task_utils.task_set.TaskSet:_sync:389 | Spec file already exists on S3 (s3://jupiter-intern-projects/aarona/task_utils/spec_files/milwaukee-full-sim-2008.yaml); overwriting...\n"
     ]
    }
   ],
   "source": [
    "DRY_RUN = False# if True, no batch runs will actually be launched\n",
    "\n",
    "\n",
    "ts = TaskSet(f'{taskset_name}.yaml',sync='overwrite')\n",
    "instr = TaskInstructions(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>S3_ROOT</th>\n",
       "      <th>env</th>\n",
       "      <th>project</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-03-01</td>\n",
       "      <td>s3://jupiter-intern-projects/aarona</td>\n",
       "      <td>dev</td>\n",
       "      <td>csg</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_date                              S3_ROOT  env project    end_date  \\\n",
       "0  2008-03-01  s3://jupiter-intern-projects/aarona  dev     csg  2008-11-02   \n",
       "\n",
       "  start_hour  \n",
       "0         00  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-25 16:42:50,005 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv exists\n",
      "2022-07-25 16:42:50,012 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:assign_basic_test_set:641 | Assigned the first 1 entries as the test set\n",
      "2022-07-25 16:42:50,120 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n",
      "2022-07-25 16:42:50,144 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:filter_tests_only:218 | Subsetting instructions to only run tasks flagged as tests\n",
      "2022-07-25 16:42:50,370 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv exists\n",
      "2022-07-25 16:42:50,420 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n",
      "2022-07-25 16:42:50,425 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:preview:239 | Generating preview of milwaukee-full-sim-2008\n",
      "command_string: python /home/jupiter/model/noahmp/run-noahmp-era5.py --num-cpus 9 --ICBC-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/ICBC --number-runday 245 --LSM-timestep 900 --output-timestep 1800 --save-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/results --start-date 2008-03-01 --start-hour 00 --geogrid-file /home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc\n",
      "Specified batch infrastructure: queue csp-dev-wrf, job-def aarona-noah-mp-hue:1\n",
      "1 commands\n",
      "Notebook DRY_RUN value is set to False\n"
     ]
    }
   ],
   "source": [
    "# use this for the basic testing mode; just do the first X runs\n",
    "test_mode = 'basic'\n",
    "number_of_tests = 1\n",
    "\n",
    "# use this if you want your tests to comprise of one run for each value of a specified loop (or multiple loops)\n",
    "#test_mode = \"one_per_loop\"\n",
    "#loop_names = [\"peril\"]\n",
    "\n",
    "# use this if you want to specify a custom query to define your test set\n",
    "#test_mode = \"query\"\n",
    "#test_query = f\"(tileid in {testset}) and (projection_scenario == 'ssp585') and (metric == 'windSpeed500yr')\"#\" and scenario == 'worst_one'\"\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "load_existing_inventory_for_tests = False\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory_for_tests)\n",
    "if test_mode == 'basic': task_inv.assign_basic_test_set(number_in_set = number_of_tests)\n",
    "elif test_mode == 'one_per_loop': task_inv.assign_test_set_by_loop(loops=loop_names, combinations=True)\n",
    "elif test_mode == 'query': task_inv.assign_test_set_by_query(query_str=test_query)\n",
    "else: raise ValueError(f\"test_mode {test_mode} not valid\")\n",
    "task_inv.save()\n",
    "\n",
    "task_instructions_test = TaskInstructions(ts)\n",
    "task_instructions_test.filter_tests_only()\n",
    "task_instructions_test.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch test jobs here\n",
    "\n",
    "Please use job arrays to make this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-25 16:42:54,121 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:write_instructions:353 | Wrote instruction_file to s3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_milwaukee-full-sim-2008_20220725_164254.yaml\n",
      "2022-07-25 16:42:54,207 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:launch_via_aws_batch:256 | Launching 1 AWS Batch Jobs on queue csp-dev-wrf with job definition aarona-noah-mp-hue:1\n",
      "2022-07-25 16:42:54,208 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:789 | Submitting single job milwaukee-full-sim-2008_2008-03-01 to queue csp-dev-wrf with job definition aarona-noah-mp-hue:1\n",
      "2022-07-25 16:42:54,208 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:792 | Submission: {'jobName': 'milwaukee-full-sim-2008_2008-03-01', 'jobQueue': 'csp-dev-wrf', 'jobDefinition': 'aarona-noah-mp-hue:1', 'containerOverrides': ordereddict([('resourceRequirements', [ordereddict([('type', 'MEMORY'), ('value', '140000')]), ordereddict([('type', 'VCPU'), ('value', '72')])]), ('environment', [{'name': 'JUPITER_TASK_SET', 'value': 's3://jupiter-intern-projects/aarona/task_utils/spec_files/milwaukee-full-sim-2008.yaml'}, {'name': 'JUPITER_TASK_INSTRUCTIONS', 'value': 's3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_milwaukee-full-sim-2008_20220725_164254.yaml'}]), ('command', ['python', '/home/jupiter/model/noahmp/run-noahmp-era5.py', '--num-cpus', '9', '--ICBC-location', 's3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/ICBC', '--number-runday', '245', '--LSM-timestep', '900', '--output-timestep', '1800', '--save-location', 's3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/results', '--start-date', '2008-03-01', '--start-hour', '00', '--geogrid-file', '/home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc'])])}\n",
      "2022-07-25 16:42:54,317 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:794 | Got response: {'ResponseMetadata': {'RequestId': 'a7a8491e-f882-4ff0-a85a-f67b67474193', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 25 Jul 2022 16:42:54 GMT', 'content-type': 'application/json', 'content-length': '184', 'connection': 'keep-alive', 'x-amzn-requestid': 'a7a8491e-f882-4ff0-a85a-f67b67474193', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'V1QyQEW7IAMFgmw=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-62dec80e-2fea2d74123d64a56f8f93e9'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:us-east-1:660097632732:job/f780dd48-5280-4ab0-945c-d9bfc437d091', 'jobName': 'milwaukee-full-sim-2008_2008-03-01', 'jobId': 'f780dd48-5280-4ab0-945c-d9bfc437d091'}\n",
      "2022-07-25 16:42:54,404 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:mark_as_launched:860 | Wrote log record to s3://jupiter-intern-projects/aarona/task_utils/launch_records/20220725_164254_milwaukee-full-sim-2008.txt\n",
      "2022-07-25 16:42:54,573 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-25 16:42:54,574 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_sync:356 | TaskSet loaded from official S3 location; no sync needed\n",
      "2022-07-25 16:42:54,591 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv exists\n",
      "2022-07-25 16:42:54,628 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n",
      "2022-07-25 16:42:54,629 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_record_job_ids:153 | Updating TaskInventory (s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv) with job ids\n",
      "2022-07-25 16:42:54,674 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n"
     ]
    }
   ],
   "source": [
    "use_job_arrays = False\n",
    "job_array_split_criteria = None #'peril'\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# See step 4 for details\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file_test = task_instructions_test.write_instructions()\n",
    "tl_test = TaskLauncher(instructions_file_test)\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-25 16:57:12,884 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv exists\n",
      "2022-07-25 16:57:13,019 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n",
      "2022-07-25 16:57:13,021 | WARNING    | jupiter.task_utils.task_inventory.TaskInventory:update_status:302 | No indicators were included with the input TaskSet, so there is no way to update status.\n",
      "2022-07-25 16:57:13,021 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:455 | Updating Batch job status for milwaukee-full-sim-2008\n",
      "2022-07-25 16:57:13,029 | DEBUG      | jupiter.task_utils.task_inventory.TaskInventory:_query_batch_status:506 | Check batch status for jobs 0:100\n",
      "2022-07-25 16:57:13,158 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:457 | Batch job status update complete!\n",
      "2022-07-25 16:57:13,161 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:414 | ---------------------------------------------------------------\n",
      "2022-07-25 16:57:13,162 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:415 | Summary of TESTS for milwaukee-full-sim-2008 as of 2022-07-25 16:57:13 UTC\n",
      "2022-07-25 16:57:13,162 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:418 | Out of 1 tests...\n",
      "2022-07-25 16:57:13,162 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:427 | ---------------------------------------------------------------\n",
      "2022-07-25 16:57:13,165 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:601 | ---------------------------------------------------------------\n",
      "2022-07-25 16:57:13,165 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:602 | Batch Status of TESTS for milwaukee-full-sim-2008 as of 2022-07-25 16:57:13 UTC\n",
      "2022-07-25 16:57:13,166 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:605 | Out of 1 tests...\n",
      "2022-07-25 16:57:13,168 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:620 | RUNNING: 1 (100.0%)\n",
      "2022-07-25 16:57:13,169 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:621 | ---------------------------------------------------------------\n",
      "2022-07-25 16:57:13,237 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-full-sim-2008.csv\n"
     ]
    }
   ],
   "source": [
    "# No options here, just execute it!\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {\"completed\":1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = [\"SUCCEEDED\"]\n",
    "num_processors = None\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses, tests_only = True)\n",
    "task_inv.print_summary(tests_only = True)\n",
    "if also_update_batch_status: task_inv.print_batch_summary(tests_only = True)\n",
    "task_inv.save()\n",
    "#task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_afc20_row0_col0,#T_afc20_row0_col1,#T_afc20_row0_col2,#T_afc20_row0_col3,#T_afc20_row0_col4,#T_afc20_row0_col5,#T_afc20_row0_col6,#T_afc20_row0_col7,#T_afc20_row0_col8,#T_afc20_row0_col9,#T_afc20_row0_col10{\n",
       "            background-color:  lightskyblue;\n",
       "        }</style><table id=\"T_afc20_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >start_date</th>        <th class=\"col_heading level0 col1\" >end_date</th>        <th class=\"col_heading level0 col2\" >start_hour</th>        <th class=\"col_heading level0 col3\" >LAST_KNOWN_JOB_STATUS</th>        <th class=\"col_heading level0 col4\" >CLOUDWATCH_LOGS</th>        <th class=\"col_heading level0 col5\" >JOB_STATUS_REASON</th>        <th class=\"col_heading level0 col6\" >SPOT_TERMINATED</th>        <th class=\"col_heading level0 col7\" >IN_TEST_SET</th>        <th class=\"col_heading level0 col8\" >LAST_KNOWN_JOB_ID</th>        <th class=\"col_heading level0 col9\" >LAST_JOB_CHECK</th>        <th class=\"col_heading level0 col10\" >JOB_DURATION</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_afc20_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_afc20_row0_col0\" class=\"data row0 col0\" >2008-03-01</td>\n",
       "                        <td id=\"T_afc20_row0_col1\" class=\"data row0 col1\" >2008-11-02</td>\n",
       "                        <td id=\"T_afc20_row0_col2\" class=\"data row0 col2\" >00</td>\n",
       "                        <td id=\"T_afc20_row0_col3\" class=\"data row0 col3\" >RUNNING</td>\n",
       "                        <td id=\"T_afc20_row0_col4\" class=\"data row0 col4\" ><a target=\"_blank\" href=\"https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252F111c56eb714d4a3ea24975782798c18f\">link</a></td>\n",
       "                        <td id=\"T_afc20_row0_col5\" class=\"data row0 col5\" >UNKNOWN</td>\n",
       "                        <td id=\"T_afc20_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "                        <td id=\"T_afc20_row0_col7\" class=\"data row0 col7\" >✓</td>\n",
       "                        <td id=\"T_afc20_row0_col8\" class=\"data row0 col8\" >f780dd48-5280-4ab0-945c-d9bfc437d091</td>\n",
       "                        <td id=\"T_afc20_row0_col9\" class=\"data row0 col9\" >2022-07-25T16:52:52</td>\n",
       "                        <td id=\"T_afc20_row0_col10\" class=\"data row0 col10\" >--</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9189920b50>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252Fd4c251e3d62a42c3abd4528957bbb2ad'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.test_df['CLOUDWATCH_LOGS'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'completed != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"] and IN_TEST_SET == True'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS == \"FAILED\" and IN_TEST_SET == True and completed != 1'\n",
    "\n",
    "update_first = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "#rerun_instructions.filter_tests_only()\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_instructions_file_test = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file_test)\n",
    "use_job_arrays = False\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "use_job_arrays = True\n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR RUNNING ALL TASKS\n",
    "## Create instructions for all remaining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create instructions and verify preview\n",
    "task_instructions = TaskInstructions(ts)\n",
    "task_instructions.filter_tests_excluded()  # comment this line if you want to run EVERYTHING, even previous tests\n",
    "#task_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch all runs\n",
    "Please use job arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 5\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file = task_instructions.write_instructions()\n",
    "tl = TaskLauncher(instructions_file)\n",
    "\n",
    "if use_job_arrays: tl.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "task_inv.print_summary()\n",
    "if also_update_batch_status: task_inv.print_batch_summary()\n",
    "task_inv.save()\n",
    "#.styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_inv.apply_style(task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED']['tileid'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'succeeded != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"]'\n",
    "#filter_query = '(completed != 1)'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS in [\"FAILED\"]'\n",
    "\n",
    "update_first = False\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = False\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rerun_instructions_file = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file)\n",
    "\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "    \n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
