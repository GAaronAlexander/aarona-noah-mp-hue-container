{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task-utils for computing CSG metrics from Climate Impacts Lab downscaled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from jupiter.task_utils import TaskSet, TaskInventory, TaskInstructions, TaskLauncher\n",
    "from jupiter.aws.s3 import upload_s3_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some parameters\n",
    "\n",
    "params = {\n",
    "    # Path to inventory file\n",
    "    'freq_want': '1H',\n",
    "    'save_location':'s3://jupiter-intern-projects/aarona/noah-mp-hue/${domain}/${start_date}_${end_date}/ICBC',\n",
    "    'domain': 'milwaukee',\n",
    "    'geogrid_file':'/home/jupiter/model/noahmp/geogrid-files/geo_em.d01.${domain}.nc',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "batch_queue = 'csp-dev-wrf' #special queue for me\n",
    "taskset_name = 'milwaukee-2008-test' # this is called aarona-noah-mp-hue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find expected output files\n",
    "Load the inventory file to figure out how many output files we need to loop over/should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = ['2008-03-01']\n",
    "end_dates = ['2008-11-02']\n",
    "\n",
    "params['start_dates'] = start_dates\n",
    "params['end_dates'] = end_dates\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting task-utils YAML\n",
    "\n",
    "This dictionary should include the key components to the task-utils yaml file.  The lists of download parameters from the dataframe above are programmatically inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /home/jupiter/model/noahmp/generate-era5-boundary-conditions.py --start-date ${start_date} --end-date ${end_date} --freq ${freq_want} --save-location ${save_location} --geogrid-file ${geogrid_file}\n"
     ]
    }
   ],
   "source": [
    "cmd_str = 'python /home/jupiter/model/noahmp/generate-era5-boundary-conditions.py --start-date ${start_date} --end-date ${end_date} --freq ${freq_want} --save-location ${save_location} --geogrid-file ${geogrid_file}'\n",
    "print(cmd_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\n",
    "    'name' : taskset_name,\n",
    "    'labels' : {\n",
    "        'env' : 'dev',\n",
    "        'project' : 'csg',\n",
    "        'S3_ROOT' : 's3://jupiter-intern-projects/aarona', #going to point to my s3 space\n",
    "    },\n",
    "    'definitions' : params,\n",
    "    'launch_settings' : {\n",
    "        'batch' : {\n",
    "            #'job_def':f'eos-external-data-etl:3',\n",
    "            'job_def': f'aarona-noah-mp-hue:1',\n",
    "            'queue' : batch_queue,\n",
    "            #'overrides' : {\n",
    "            #    'vcpus' : 4,\n",
    "            #    'memory' : 16000,\n",
    "            #},\n",
    "            'overrides' : {\n",
    "                \"resourceRequirements\": [\n",
    "                    {\n",
    "                        \"type\": \"MEMORY\",\n",
    "                        \"value\" : \"16000\" #MB\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"VCPU\",\n",
    "                        \"value\" : \"4\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'run_keys' : {\n",
    "            'command_string' : cmd_str\n",
    "        }\n",
    "    },\n",
    "    'indicators' : {\n",
    "        #'completed' : {\n",
    "        #    'components' : ['${outpath}'],\n",
    "        #   'method' : 's3_sensor'\n",
    "       # },\n",
    "    },\n",
    "    'loops': {\n",
    "        'start_date' : 'start_dates',\n",
    "    },\n",
    "    'mapped_loops' : {\n",
    "        'end_date' : {'start_date' : 'end_dates'},\n",
    "       # 'gcm' : {'job_index' : 'gcms'},\n",
    "       # 'scenario' : {'job_index' : 'scenarios'},\n",
    "       # 'year' : {'job_index' : 'years'}\n",
    "    }\n",
    "        \n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a yaml file\n",
    "with open(f'{taskset_name}.yaml', 'w') as outfile:\n",
    "    docs = yaml.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with task-utils\n",
    "\n",
    "This follows the normal task-utils sequence.  Start by making an instruction list (which should also help confirm our YAML was formatted correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-22 19:37:00,280 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-22 19:37:00,346 | INFO       | jupiter.task_utils.task_set.TaskSet:_sync:393 | TaskSet does not yet exist on S3; saving to s3://jupiter-intern-projects/aarona/task_utils/spec_files/milwaukee-2008-test.yaml\n"
     ]
    }
   ],
   "source": [
    "DRY_RUN = False# if True, no batch runs will actually be launched\n",
    "\n",
    "\n",
    "ts = TaskSet(f'{taskset_name}.yaml',sync='overwrite')\n",
    "instr = TaskInstructions(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>S3_ROOT</th>\n",
       "      <th>env</th>\n",
       "      <th>project</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-03-01</td>\n",
       "      <td>s3://jupiter-intern-projects/aarona</td>\n",
       "      <td>dev</td>\n",
       "      <td>csg</td>\n",
       "      <td>2008-11-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_date                              S3_ROOT  env project    end_date\n",
       "0  2008-03-01  s3://jupiter-intern-projects/aarona  dev     csg  2008-11-02"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-22 19:37:05,562 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:assign_basic_test_set:641 | Assigned the first 1 entries as the test set\n",
      "2022-07-22 19:37:05,656 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n",
      "2022-07-22 19:37:05,688 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:filter_tests_only:218 | Subsetting instructions to only run tasks flagged as tests\n",
      "2022-07-22 19:37:05,729 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv exists\n",
      "2022-07-22 19:37:05,774 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n",
      "2022-07-22 19:37:05,780 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:preview:239 | Generating preview of milwaukee-2008-test\n",
      "command_string: python /home/jupiter/model/noahmp/generate-era5-boundary-conditions.py --start-date 2008-03-01 --end-date 2008-11-02 --freq 1H --save-location s3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/ICBC --geogrid-file /home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc\n",
      "Specified batch infrastructure: queue csp-dev-wrf, job-def aarona-noah-mp-hue:1\n",
      "1 commands\n",
      "Notebook DRY_RUN value is set to False\n"
     ]
    }
   ],
   "source": [
    "# use this for the basic testing mode; just do the first X runs\n",
    "test_mode = 'basic'\n",
    "number_of_tests = 1\n",
    "\n",
    "# use this if you want your tests to comprise of one run for each value of a specified loop (or multiple loops)\n",
    "#test_mode = \"one_per_loop\"\n",
    "#loop_names = [\"peril\"]\n",
    "\n",
    "# use this if you want to specify a custom query to define your test set\n",
    "#test_mode = \"query\"\n",
    "#test_query = f\"(tileid in {testset}) and (projection_scenario == 'ssp585') and (metric == 'windSpeed500yr')\"#\" and scenario == 'worst_one'\"\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "load_existing_inventory_for_tests = True\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory_for_tests)\n",
    "if test_mode == 'basic': task_inv.assign_basic_test_set(number_in_set = number_of_tests)\n",
    "elif test_mode == 'one_per_loop': task_inv.assign_test_set_by_loop(loops=loop_names, combinations=True)\n",
    "elif test_mode == 'query': task_inv.assign_test_set_by_query(query_str=test_query)\n",
    "else: raise ValueError(f\"test_mode {test_mode} not valid\")\n",
    "task_inv.save()\n",
    "\n",
    "task_instructions_test = TaskInstructions(ts)\n",
    "task_instructions_test.filter_tests_only()\n",
    "task_instructions_test.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch test jobs here\n",
    "\n",
    "Please use job arrays to make this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-22 19:37:11,156 | INFO       | jupiter.task_utils.task_instructions.TaskInstructions:write_instructions:353 | Wrote instruction_file to s3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_milwaukee-2008-test_20220722_193711.yaml\n",
      "2022-07-22 19:37:11,249 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:launch_via_aws_batch:256 | Launching 1 AWS Batch Jobs on queue csp-dev-wrf with job definition aarona-noah-mp-hue:1\n",
      "2022-07-22 19:37:11,251 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:789 | Submitting single job milwaukee-2008-test_2008-03-01 to queue csp-dev-wrf with job definition aarona-noah-mp-hue:1\n",
      "2022-07-22 19:37:11,252 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:792 | Submission: {'jobName': 'milwaukee-2008-test_2008-03-01', 'jobQueue': 'csp-dev-wrf', 'jobDefinition': 'aarona-noah-mp-hue:1', 'containerOverrides': ordereddict([('resourceRequirements', [ordereddict([('type', 'MEMORY'), ('value', '16000')]), ordereddict([('type', 'VCPU'), ('value', '4')])]), ('environment', [{'name': 'JUPITER_TASK_SET', 'value': 's3://jupiter-intern-projects/aarona/task_utils/spec_files/milwaukee-2008-test.yaml'}, {'name': 'JUPITER_TASK_INSTRUCTIONS', 'value': 's3://jupiter-intern-projects/aarona/task_utils/instruction_files/instructions_milwaukee-2008-test_20220722_193711.yaml'}]), ('command', ['python', '/home/jupiter/model/noahmp/generate-era5-boundary-conditions.py', '--start-date', '2008-03-01', '--end-date', '2008-11-02', '--freq', '1H', '--save-location', 's3://jupiter-intern-projects/aarona/noah-mp-hue/milwaukee/2008-03-01_2008-11-02/ICBC', '--geogrid-file', '/home/jupiter/model/noahmp/geogrid-files/geo_em.d01.milwaukee.nc'])])}\n",
      "2022-07-22 19:37:11,369 | DEBUG      | jupiter.task_utils.task_launcher.TaskLauncher:_submit_single_batch_job:794 | Got response: {'ResponseMetadata': {'RequestId': '9eaa1d64-e20a-4c2f-a11b-81310b0df15c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 22 Jul 2022 19:37:11 GMT', 'content-type': 'application/json', 'content-length': '180', 'connection': 'keep-alive', 'x-amzn-requestid': '9eaa1d64-e20a-4c2f-a11b-81310b0df15c', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'VrxgKGVxIAMFW8g=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-62dafc67-4f4fed28281106ff1d58cce9'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:us-east-1:660097632732:job/8ea59673-01c7-4de1-8781-9eb7d9eaae9e', 'jobName': 'milwaukee-2008-test_2008-03-01', 'jobId': '8ea59673-01c7-4de1-8781-9eb7d9eaae9e'}\n",
      "2022-07-22 19:37:11,483 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:mark_as_launched:860 | Wrote log record to s3://jupiter-intern-projects/aarona/task_utils/launch_records/20220722_193711_milwaukee-2008-test.txt\n",
      "2022-07-22 19:37:11,579 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_create_s3_root_definition:286 | S3_ROOT is already defined: s3://jupiter-intern-projects/aarona\n",
      "2022-07-22 19:37:11,580 | DEBUG      | jupiter.task_utils.task_set.TaskSet:_sync:356 | TaskSet loaded from official S3 location; no sync needed\n",
      "2022-07-22 19:37:11,598 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv exists\n",
      "2022-07-22 19:37:11,645 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n",
      "2022-07-22 19:37:11,646 | INFO       | jupiter.task_utils.task_launcher.TaskLauncher:_record_job_ids:153 | Updating TaskInventory (s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv) with job ids\n",
      "2022-07-22 19:37:11,708 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n"
     ]
    }
   ],
   "source": [
    "use_job_arrays = False\n",
    "job_array_split_criteria = None #'peril'\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# See step 4 for details\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file_test = task_instructions_test.write_instructions()\n",
    "tl_test = TaskLauncher(instructions_file_test)\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-22 19:39:22,896 | DEBUG      | jupiter.task_utils.concurrent_s3_client:check_s3_object_exists:161 | Verified that object s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv exists\n",
      "2022-07-22 19:39:22,988 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:_initialize_df:156 | Loaded inventory from s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n",
      "2022-07-22 19:39:22,989 | WARNING    | jupiter.task_utils.task_inventory.TaskInventory:update_status:302 | No indicators were included with the input TaskSet, so there is no way to update status.\n",
      "2022-07-22 19:39:22,990 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:455 | Updating Batch job status for milwaukee-2008-test\n",
      "2022-07-22 19:39:23,001 | DEBUG      | jupiter.task_utils.task_inventory.TaskInventory:_query_batch_status:506 | Check batch status for jobs 0:100\n",
      "2022-07-22 19:39:23,316 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:update_batch_status:457 | Batch job status update complete!\n",
      "2022-07-22 19:39:23,319 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:414 | ---------------------------------------------------------------\n",
      "2022-07-22 19:39:23,320 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:415 | Summary of TESTS for milwaukee-2008-test as of 2022-07-22 19:39:23 UTC\n",
      "2022-07-22 19:39:23,321 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:418 | Out of 1 tests...\n",
      "2022-07-22 19:39:23,323 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_summary:427 | ---------------------------------------------------------------\n",
      "2022-07-22 19:39:23,325 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:601 | ---------------------------------------------------------------\n",
      "2022-07-22 19:39:23,326 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:602 | Batch Status of TESTS for milwaukee-2008-test as of 2022-07-22 19:39:23 UTC\n",
      "2022-07-22 19:39:23,327 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:605 | Out of 1 tests...\n",
      "2022-07-22 19:39:23,331 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:620 | RUNNABLE: 1 (100.0%)\n",
      "2022-07-22 19:39:23,332 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:print_batch_summary:621 | ---------------------------------------------------------------\n",
      "2022-07-22 19:39:23,397 | INFO       | jupiter.task_utils.task_inventory.TaskInventory:save:387 | Saved inventory to s3://jupiter-intern-projects/aarona/task_utils/inventories/milwaukee-2008-test.csv\n"
     ]
    }
   ],
   "source": [
    "# No options here, just execute it!\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {\"completed\":1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = [\"SUCCEEDED\"]\n",
    "num_processors = None\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses, tests_only = True)\n",
    "task_inv.print_summary(tests_only = True)\n",
    "if also_update_batch_status: task_inv.print_batch_summary(tests_only = True)\n",
    "task_inv.save()\n",
    "#task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_adcdb_row0_col0,#T_adcdb_row0_col1,#T_adcdb_row0_col2,#T_adcdb_row0_col3,#T_adcdb_row0_col4,#T_adcdb_row0_col5,#T_adcdb_row0_col6,#T_adcdb_row0_col7,#T_adcdb_row0_col8,#T_adcdb_row0_col9{\n",
       "            background-color:  #dce6fc;\n",
       "        }</style><table id=\"T_adcdb_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >start_date</th>        <th class=\"col_heading level0 col1\" >end_date</th>        <th class=\"col_heading level0 col2\" >LAST_KNOWN_JOB_STATUS</th>        <th class=\"col_heading level0 col3\" >CLOUDWATCH_LOGS</th>        <th class=\"col_heading level0 col4\" >JOB_STATUS_REASON</th>        <th class=\"col_heading level0 col5\" >SPOT_TERMINATED</th>        <th class=\"col_heading level0 col6\" >IN_TEST_SET</th>        <th class=\"col_heading level0 col7\" >LAST_KNOWN_JOB_ID</th>        <th class=\"col_heading level0 col8\" >LAST_JOB_CHECK</th>        <th class=\"col_heading level0 col9\" >JOB_DURATION</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_adcdb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_adcdb_row0_col0\" class=\"data row0 col0\" >2008-03-01</td>\n",
       "                        <td id=\"T_adcdb_row0_col1\" class=\"data row0 col1\" >2008-11-02</td>\n",
       "                        <td id=\"T_adcdb_row0_col2\" class=\"data row0 col2\" >RUNNABLE</td>\n",
       "                        <td id=\"T_adcdb_row0_col3\" class=\"data row0 col3\" >--</td>\n",
       "                        <td id=\"T_adcdb_row0_col4\" class=\"data row0 col4\" >UNKNOWN</td>\n",
       "                        <td id=\"T_adcdb_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "                        <td id=\"T_adcdb_row0_col6\" class=\"data row0 col6\" >✓</td>\n",
       "                        <td id=\"T_adcdb_row0_col7\" class=\"data row0 col7\" >8ea59673-01c7-4de1-8781-9eb7d9eaae9e</td>\n",
       "                        <td id=\"T_adcdb_row0_col8\" class=\"data row0 col8\" >2022-07-22T19:38:44</td>\n",
       "                        <td id=\"T_adcdb_row0_col9\" class=\"data row0 col9\" >--</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff1c0b21f10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.apply_style(task_inv.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/%252Faws%252Fbatch%252Fjob/log-events/aarona-noah-mp-hue%252Fdefault%252F9a167c2522104009a1133d532de65771'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_inv.test_df['CLOUDWATCH_LOGS'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures of test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'completed != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"] and IN_TEST_SET == True'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS == \"FAILED\" and IN_TEST_SET == True and completed != 1'\n",
    "\n",
    "update_first = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "#rerun_instructions.filter_tests_only()\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_instructions_file_test = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file_test)\n",
    "use_job_arrays = False\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "use_job_arrays = True\n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR RUNNING ALL TASKS\n",
    "## Create instructions for all remaining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create instructions and verify preview\n",
    "task_instructions = TaskInstructions(ts)\n",
    "task_instructions.filter_tests_excluded()  # comment this line if you want to run EVERYTHING, even previous tests\n",
    "#task_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch all runs\n",
    "Please use job arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 5\n",
    "group_criteria = None\n",
    "\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "instructions_file = task_instructions.write_instructions()\n",
    "tl = TaskLauncher(instructions_file)\n",
    "\n",
    "if use_job_arrays: tl.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_inventory = True\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = True\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "task_inv = TaskInventory(ts, load_existing = load_existing_inventory)\n",
    "task_inv.update_status(skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "task_inv.print_summary()\n",
    "if also_update_batch_status: task_inv.print_batch_summary()\n",
    "task_inv.save()\n",
    "#.styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_inv.apply_style(task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inv.df[task_inv.df['LAST_KNOWN_JOB_STATUS']=='FAILED']['tileid'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter_query = 'succeeded != 1 and LAST_KNOWN_JOB_STATUS not in [\"SUBMITTED\",\"PENDING\",\"STARTING\",\"RUNNABLE\",\"RUNNING\"]'\n",
    "#filter_query = '(completed != 1)'\n",
    "filter_query = 'LAST_KNOWN_JOB_STATUS in [\"FAILED\"]'\n",
    "\n",
    "update_first = False\n",
    "skip_s3_datacheck = {'completed': 1}\n",
    "also_update_batch_status = False\n",
    "batch_skip_statuses = ['SUCCEEDED']\n",
    "num_processors = None\n",
    "\n",
    "###################  ↑↑   OPTIONS   ↑↑  ###################\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "###################  ↓↓ DON'T TOUCH ↓↓  ###################\n",
    "\n",
    "rerun_instructions = TaskInstructions(ts)\n",
    "rerun_instructions.filter_on_inventory(query = filter_query, update_first=update_first, skip_values = skip_s3_datacheck, also_update_batch_status = also_update_batch_status, nproc=num_processors, batch_skip_statuses = batch_skip_statuses)\n",
    "rerun_instructions.preview()\n",
    "print(f'Notebook DRY_RUN value is set to {DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_job_arrays = True\n",
    "job_array_split_criteria = None # example only, adjust for your use case\n",
    "\n",
    "## Advanced options, please do not use rashly!\n",
    "num_attempts = None\n",
    "timeout = None\n",
    "\n",
    "# Set one of these options to run multiple commands in a loop\n",
    "# within a single container execution. Your container may need \n",
    "# special code to handle this properly!\n",
    "jobs_per_execution = 1\n",
    "group_criteria = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rerun_instructions_file = rerun_instructions.write_instructions()\n",
    "tl_test = TaskLauncher(rerun_instructions_file)\n",
    "\n",
    "if use_job_arrays: tl_test.launch_via_aws_job_array(dry_run = DRY_RUN, split_on = job_array_split_criteria, jobs_per_execution = jobs_per_execution, group_on = group_criteria, num_attempts = num_attempts, timeout = timeout)\n",
    "else: tl_test.launch_via_aws_batch(dry_run = DRY_RUN, num_attempts = num_attempts, timeout = timeout)\n",
    "    \n",
    "# Go back up and re-check the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
